{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d9e446",
   "metadata": {},
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org and make data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc5b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c770429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    headers.append(header.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85c36d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Headers': headers})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105fadf",
   "metadata": {},
   "source": [
    "# Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f737795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9648fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2ee40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the details from the website\n",
    "names = []\n",
    "terms = []\n",
    "for row in soup.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    name = cols[0].text.strip()\n",
    "    term = cols[1].text.strip()\n",
    "    names.append(name)\n",
    "    terms.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fc74e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame\n",
    "data = {'Name': names, 'Term of Office': terms}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6af35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff464d",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9bfd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46a03a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2bfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_table = soup.find('table', class_='table')\n",
    "team_rows = team_table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d306fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = []\n",
    "for row in team_rows[1:11]:\n",
    "    team_data = row.find_all('td')\n",
    "    rank = team_data[0].text.strip()\n",
    "    team = team_data[1].text.strip()\n",
    "    matches = team_data[2].text.strip()\n",
    "    points = team_data[3].text.strip()\n",
    "    rating = team_data[4].text.strip()\n",
    "    teams.append([rank, team, matches, points, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3baee849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank              Team Matches Points Rating\n",
      "0    1    Australia\\nAUS      23  2,714    118\n",
      "1    2     Pakistan\\nPAK      20  2,316    116\n",
      "2    3        India\\nIND      33  3,807    115\n",
      "3    4   New Zealand\\nNZ      27  2,806    104\n",
      "4    5      England\\nENG      24  2,426    101\n",
      "5    6  South Africa\\nSA      19  1,910    101\n",
      "6    7   Bangladesh\\nBAN      25  2,451     98\n",
      "7    8     Sri Lanka\\nSL      28  2,378     85\n",
      "8    9  Afghanistan\\nAFG      13  1,067     82\n",
      "9   10   West Indies\\nWI      32  2,201     69\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(teams, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3d6f6",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894a73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01811bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af2cfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_table = soup.find('table', class_='table')\n",
    "player_rows = player_table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "304d6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = []\n",
    "for row in player_rows[1:11]:\n",
    "    player_data = row.find_all('td')\n",
    "    rank = player_data[0].text.strip()\n",
    "    name = player_data[1].text.strip()\n",
    "    team = player_data[2].text.strip()\n",
    "    rating = player_data[3].text.strip()\n",
    "    players.append([rank, name, team, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c21bfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Rank                   Name  \\\n",
      "0               1\\n                        \\n\\n\\n(0)             Babar Azam   \n",
      "1       2\\n                                \\n\\n\\n(0)  Rassie van der Dussen   \n",
      "2       3\\n                                \\n\\n\\n(0)           Fakhar Zaman   \n",
      "3       4\\n                                \\n\\n\\n(0)            Imam-ul-Haq   \n",
      "4       5\\n                                \\n\\n\\n(0)           Shubman Gill   \n",
      "5       6\\n                                \\n\\n\\n(0)           David Warner   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...           Harry Tector   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...            Virat Kohli   \n",
      "8  9\\n                                \\n\\n\\n\\n\\n(...        Quinton de Kock   \n",
      "9      10\\n                                \\n\\n\\n(0)           Rohit Sharma   \n",
      "\n",
      "  Team Rating  \n",
      "0  PAK    886  \n",
      "1   SA    777  \n",
      "2  PAK    755  \n",
      "3  PAK    745  \n",
      "4  IND    738  \n",
      "5  AUS    726  \n",
      "6  IRE    723  \n",
      "7  IND    719  \n",
      "8   SA    718  \n",
      "9  IND    707  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(players, columns=['Rank', 'Name', 'Team', 'Rating'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235a64a",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5673a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3f2ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8715252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_table = soup.find('table', class_='table')\n",
    "player_rows = player_table.find_all('tr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3834b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = []\n",
    "for row in player_rows[1:11]:\n",
    "    player_data = row.find_all('td')\n",
    "    rank = player_data[0].text.strip()\n",
    "    name = player_data[1].text.strip()\n",
    "    team = player_data[2].text.strip()\n",
    "    rating = player_data[3].text.strip()\n",
    "    players.append([rank, name, team, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88fd767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Rank              Name Team Rating\n",
      "0           1\\n                        \\n\\n\\n(0)    Josh Hazlewood  AUS    705\n",
      "1   2\\n                                \\n\\n\\n(0)    Mohammed Siraj  IND    691\n",
      "2   3\\n                                \\n\\n\\n(0)    Mitchell Starc  AUS    686\n",
      "3   4\\n                                \\n\\n\\n(0)        Matt Henry   NZ    667\n",
      "4   5\\n                                \\n\\n\\n(0)       Trent Boult   NZ    660\n",
      "5   6\\n                                \\n\\n\\n(0)        Adam Zampa  AUS    652\n",
      "6   7\\n                                \\n\\n\\n(0)       Rashid Khan  AFG    640\n",
      "7   8\\n                                \\n\\n\\n(0)    Shaheen Afridi  PAK    630\n",
      "8   =\\n                                \\n\\n\\n(0)  Mujeeb Ur Rahman  AFG    630\n",
      "9  10\\n                                \\n\\n\\n(0)     Mohammad Nabi  AFG    626\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(players, columns=['Rank', 'Name', 'Team', 'Rating'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5adc958",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d282d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46226d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbe7ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_table = soup.find('table', class_='table')\n",
    "\n",
    "headers = [header.text for header in team_table.find_all('th')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae6a65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = team_table.find_all('tr')[1:11]\n",
    "row_data = [[td.text.strip() for td in row.find_all('td')] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db1bb1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pos       \\nTeam\\nT\\n \\nMatches\\nM\\n \\nPoints\\nP\\n \\nRating\\nR\\n\n",
      "0   1    Australia\\nAUS             21         3,603           172\n",
      "1   2      England\\nENG             28         3,342           119\n",
      "2   3  South Africa\\nSA             26         3,098           119\n",
      "3   4        India\\nIND             27         2,820           104\n",
      "4   5   New Zealand\\nNZ             25         2,553           102\n",
      "5   6   West Indies\\nWI             27         2,535            94\n",
      "6   7     Thailand\\nTHA             11           821            75\n",
      "7   8   Bangladesh\\nBAN             14           977            70\n",
      "8   9     Pakistan\\nPAK             27         1,678            62\n",
      "9  10     Sri Lanka\\nSL              9           479            53\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(row_data, columns=headers)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ade06",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9960cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b32ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fedbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_table = soup.find('table', class_='table')\n",
    "\n",
    "headers = [header.text for header in player_table.find_all('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "935f774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = player_table.find_all('tr')[1:11]\n",
    "row_data = [[td.text.strip() for td in row.find_all('td')] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee3e461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Pos               Player  \\\n",
      "0  1\\n                        \\n\\n\\n\\n\\n(1)\\nThis...          Beth Mooney   \n",
      "1  2\\n                                \\n\\n\\n\\n\\n(...      Laura Wolvaardt   \n",
      "2  3\\n                                \\n\\n\\n\\n\\n(...       Natalie Sciver   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...          Meg Lanning   \n",
      "4  5\\n                                \\n\\n\\n\\n\\n(...     Harmanpreet Kaur   \n",
      "5  6\\n                                \\n\\n\\n\\n\\n(...      Smriti Mandhana   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...  Chamari Athapaththu   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...         Ellyse Perry   \n",
      "8  9\\n                                \\n\\n\\n\\n\\n(...       Tammy Beaumont   \n",
      "9  10\\n                                \\n\\n\\n\\n\\n...      Stafanie Taylor   \n",
      "\n",
      "  Team Rating              Career Best Rating  \n",
      "0  AUS    754      754 v Pakistan, 21/01/2023  \n",
      "1   SA    732     741 v Australia, 22/03/2022  \n",
      "2  ENG    731  755 v South Africa, 15/07/2022  \n",
      "3  AUS    717   834 v New Zealand, 24/02/2016  \n",
      "4  IND    716       731 v England, 21/09/2022  \n",
      "5  IND    714       797 v England, 28/02/2019  \n",
      "6   SL    673  691 v South Africa, 14/02/2019  \n",
      "7  AUS    626   766 v West Indies, 11/09/2019  \n",
      "8  ENG    595         791 v India, 27/06/2021  \n",
      "9   WI    588      766 v Pakistan, 07/07/2021  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(row_data, columns=headers)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ffdfa",
   "metadata": {},
   "source": [
    "# c)Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c39c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5610d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94180c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_table = soup.find('table', class_='table')\n",
    "\n",
    "headers = [header.text for header in player_table.find_all('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7837bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = player_table.find_all('tr')[1:11]\n",
    "row_data = [[td.text.strip() for td in row.find_all('td')] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7477be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Pos               Player  \\\n",
      "0  1\\n                        \\n\\n\\n\\n\\n(1)\\nThis...          Beth Mooney   \n",
      "1  2\\n                                \\n\\n\\n\\n\\n(...      Laura Wolvaardt   \n",
      "2  3\\n                                \\n\\n\\n\\n\\n(...       Natalie Sciver   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...          Meg Lanning   \n",
      "4  5\\n                                \\n\\n\\n\\n\\n(...     Harmanpreet Kaur   \n",
      "5  6\\n                                \\n\\n\\n\\n\\n(...      Smriti Mandhana   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...  Chamari Athapaththu   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...         Ellyse Perry   \n",
      "8  9\\n                                \\n\\n\\n\\n\\n(...       Tammy Beaumont   \n",
      "9  10\\n                                \\n\\n\\n\\n\\n...      Stafanie Taylor   \n",
      "\n",
      "  Team Rating              Career Best Rating  \n",
      "0  AUS    754      754 v Pakistan, 21/01/2023  \n",
      "1   SA    732     741 v Australia, 22/03/2022  \n",
      "2  ENG    731  755 v South Africa, 15/07/2022  \n",
      "3  AUS    717   834 v New Zealand, 24/02/2016  \n",
      "4  IND    716       731 v England, 21/09/2022  \n",
      "5  IND    714       797 v England, 28/02/2019  \n",
      "6   SL    673  691 v South Africa, 14/02/2019  \n",
      "7  AUS    626   766 v West Indies, 11/09/2019  \n",
      "8  ENG    595         791 v India, 27/06/2021  \n",
      "9   WI    588      766 v Pakistan, 07/07/2021  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(row_data, columns=headers)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b224b8c",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002i) Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33d1593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd07068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a9e4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [headline.text for headline in soup.find_all('h3', class_='Card-title')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5c1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(headlines, columns=['Headline'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90283689",
   "metadata": {},
   "source": [
    "# ii) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7437333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29145025",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e7bee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = [timestamp['datetime'] for timestamp in soup.find_all('time', class_='Card-time')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37b44c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Timestamp]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(timestamps, columns=['Timestamp'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b872a",
   "metadata": {},
   "source": [
    "# iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e91fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d23a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "015f74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.cnbc.com\"\n",
    "news_links = [base_url + link['href'] for link in soup.find_all('a', class_='Card-titleLink')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d7d2dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(news_links, columns=['News Link'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a707a",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002i) Paper Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "710f9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10f1fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06abbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('div', class_='pod-listing-header')\n",
    "titles = [article.find('a').text for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d4774fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Paper Title': titles})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a880787",
   "metadata": {},
   "source": [
    "ii) Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "263cd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09e36f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c533877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('div', class_='pod-listing-header')\n",
    "titles = [article.find('a').text for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5bf1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_list = []\n",
    "for article in articles:\n",
    "    authors = article.find_all('span', class_='text given-name')\n",
    "    authors_list.append(', '.join([author.text for author in authors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bb24668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Paper Title': titles, 'Authors': authors_list})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64ec10",
   "metadata": {},
   "source": [
    "iii) Published Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40bde664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8109230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1556234",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('div', class_='pod-listing-header')\n",
    "titles = [article.find('a').text for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b2b5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_list = []\n",
    "for article in articles:\n",
    "    authors = article.find_all('span', class_='text given-name')\n",
    "    authors_list.append(', '.join([author.text for author in authors]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95b894c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [article.find('div', class_='pod-listing-date').text for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f049f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Paper Title': titles, 'Authors': authors_list, 'Published Date': dates})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fa733",
   "metadata": {},
   "source": [
    "iv)Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5420b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75cf663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e8ff669",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = soup.find_all('div', class_='pod-listing-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7c839a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "downloads = []\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff5e0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in article_list:\n",
    "    title = article.find('h4').text.strip()\n",
    "    titles.append(title)\n",
    "    \n",
    "    author = article.find('ul', class_='author-group').text.strip()\n",
    "    authors.append(author)\n",
    "    \n",
    "    date = article.find('div', class_='date').text.strip()\n",
    "    dates.append(date)\n",
    "    \n",
    "    download = article.find('div', class_='downloads').text.strip()\n",
    "    downloads.append(download)\n",
    "    \n",
    "    url = article.find('a')['href']\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a87c5b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Author, Date, Downloads, URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = {'Title': titles, 'Author': authors, 'Date': dates, 'Downloads': downloads, 'URL': urls}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76dbad",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d951e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "993ef4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eaaf545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_names = []\n",
    "for restaurant in soup.find_all('div', class_='restnt-info cursor'):\n",
    "    name = restaurant.find('a', class_='restnt-name ellipsis').text.strip()\n",
    "    restaurant_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d00f2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name\n",
      "0                  Ministry Of Beer\n",
      "1                             Local\n",
      "2                           Tamasha\n",
      "3                       Station Bar\n",
      "4                     My Bar Square\n",
      "5                    Warehouse Cafe\n",
      "6                    Openhouse Cafe\n",
      "7                Lord of the Drinks\n",
      "8                     The G.T. Road\n",
      "9              Connaught Club House\n",
      "10              Unplugged Courtyard\n",
      "11                The Junkyard Cafe\n",
      "12                          Berco's\n",
      "13             Connaught Club House\n",
      "14                      38 Barracks\n",
      "15                              QBA\n",
      "16  Ardor 2.1 Restaurant and Lounge\n",
      "17                      Cafe High 5\n",
      "18                      Dasaprakash\n",
      "19              My Bar Headquarters\n",
      "20                           Sandoz\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(restaurant_names, columns=['Restaurant Name'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd30018",
   "metadata": {},
   "source": [
    "ii) Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "28336c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "799ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5dacf462",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisines = []\n",
    "for cuisine in soup.find_all('div', class_='cuisine'):\n",
    "    cuisines.append(cuisine.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ced1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Cuisine]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(cuisines, columns=['Cuisine'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bc8e1",
   "metadata": {},
   "source": [
    "\n",
    "iii) Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c890885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aae58b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d038c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "for location in soup.find_all('div', class_='location'):\n",
    "    locations.append(location.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "468eaba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(locations, columns=['Location'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff65cb9",
   "metadata": {},
   "source": [
    "\n",
    "iv) Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37b7816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dd8d4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50d03ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the details from the website\n",
    "details = []\n",
    "for detail in soup.find_all('div', class_='detail'):\n",
    "    detail_text = detail.text.strip()\n",
    "    details.append(detail_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7da764f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame\n",
    "df = pd.DataFrame(details, columns=['Details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d20070f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Details]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filter the data frame to only include rows with ratings\n",
    "df = df[df['Details'].str.contains('ratings')]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b146269",
   "metadata": {},
   "source": [
    "\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ad8f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "280eb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f35e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the image URLs from the website\n",
    "image_urls = []\n",
    "for img in soup.find_all('img'):\n",
    "    image_url = img.get('src')\n",
    "    image_urls.append(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "36bbb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame\n",
    "df = pd.DataFrame(image_urls, columns=['Image URL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "35dcbcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Image URL\n",
      "0  https://im1.dineout.co.in/images/uploads/misc/...\n",
      "1                                               None\n",
      "2                                               None\n",
      "3                                               None\n",
      "4                                               None\n",
      "5                                               None\n",
      "6                                               None\n",
      "7                                               None\n",
      "8                                               None\n",
      "9  https://im1.dineout.co.in/images/uploads/misc/...\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2188136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
